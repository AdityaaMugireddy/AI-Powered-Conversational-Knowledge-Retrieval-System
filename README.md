# AI-Powered-Conversational-Knowledge-Retrieval-System
This project is an AI-driven chatbot that uses retrieval-augmented generation (RAG) to answer queries based on stored documents. Built using LangChain, Ollama, ChromaDB, and DeepSeek 7B, it provides context-aware responses with structured memory for improved long-term retention.

**ğŸ”¹ Features**

**Retrieval-Augmented Generation (RAG)** â€“ Uses ChromaDB to fetch relevant documents before generating responses.

**Conversational Memory** â€“ Retains chat history using LangChainâ€™s Buffer Memory, ensuring contextual continuity.

**DeepSeek 7B LLM**â€“ Generates human-like responses with optimized reasoning.

**Fast & Efficient Retrieval** â€“ Utilizes ChromaDB for high-speed vector search of stored documents.

**Python-Based Implementation**â€“ Fully developed in Python, enabling local inference without cloud dependencies.


**ğŸ› ï¸ Tech Stack**
**Language Model:** DeepSeek 7B via Ollama

**Embedding Model:** Sentence-Transformers (MiniLM L6-v2)

**Vector Database:** ChromaDB

**Memory Management:** LangChain's Conversation Buffer

**ğŸš€ How It Works**

**1ï¸âƒ£ Load Documents**

Documents are vectorized using HuggingFace embeddings and stored in ChromaDB.

The retriever fetches the most relevant document chunks based on user queries.

**2ï¸âƒ£ Memory & Context Handling**

LangChainâ€™s Conversation Buffer Memory stores past interactions.

This allows for context-aware responses instead of isolated answers.

**3ï¸âƒ£ Query Execution**

The query is sent to the retrieval system, which fetches relevant documents.

DeepSeek 7B then generates an AI response, leveraging the retrieved context.

**4ï¸âƒ£ AI Response Generation**

The AI formats the response, including:

Generated Answer

Relevant Sources

**ğŸ”§ Future Improvements**

ğŸ”¹ Add multi-user session memory

ğŸ”¹ Enhance retrieval filtering for better accuracy

ğŸ”¹ Implement fine-tuning for domain-specific knowledge

ğŸ”¹ Integrate Agentic Workflow
